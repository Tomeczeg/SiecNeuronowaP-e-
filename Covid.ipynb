{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8E5VbYmaZHg",
        "colab_type": "code",
        "outputId": "a0b965c8-87ce-47b3-ab18-71986cd1ae82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive\n",
        "%cd '/gdrive/My Drive/Colab Notebooks/baza'\n",
        "\n",
        "!pip show tensorflow\n",
        "\n",
        "!pip show keras\n",
        "import csv\n",
        "#from tensorflow import keras\n",
        "import glob, os\n",
        "\n",
        "# For the current version: \n",
        "#!pip install --upgrade tensorflow\n",
        "\n",
        "# For a specific version:\n",
        "!pip install tensorflow==1.14\n",
        "\n",
        "!pip install keras==2.2.4\n",
        "\n",
        "#Bibioteki do obliczen tensorowych\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#import plaidml.keras\n",
        "#plaidml.keras.install_backend()\n",
        "\n",
        "#Bibioteka do obsługi sieci neuronowych\n",
        "import keras\n",
        "\n",
        "#Załadowania bazy uczącej\n",
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "Count= 70\n",
        "Size= 7\n",
        "Data_size=2\n",
        "BazaVec = np.empty((Count,Size,1))\n",
        "BazaAns = np.empty((Count,1))\n",
        "table={}\n",
        "i=0\n",
        "start_pl_index=12540\n",
        "end_pl_index=12609\n",
        "with open('data.csv', newline='', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "        table[i]=row\n",
        "        i=i+1\n",
        "z1=0\n",
        "for l in range(start_pl_index,end_pl_index-6):\n",
        "    for j in range(7):\n",
        "        BazaVec[z1,j,:]=float(table[l+j][4]) # dzienny przyrost \n",
        "    BazaAns[z1]=float(table[l+7][4])\n",
        "    z1=z1+1\n",
        "max_answer=np.amax(BazaAns)/2\n",
        "print(max_answer)\n",
        "exit()\n",
        "\n",
        "BazaAns=BazaAns/max_answer-1\n",
        "BazaAns=BazaAns[0:z1]\n",
        "maxval = np.amax(BazaVec)/2\n",
        "BazaVec=BazaVec/max_answer-1\n",
        "BazaVec=BazaVec[0:z1,:,:]\n",
        "#\n",
        "##Stworzenia modelu sieci\n",
        "inputt = keras.engine.input_layer.Input(shape=(Size,1),name=\"wejscie\")\n",
        "#\n",
        "FlattenLayer = keras.layers.Flatten()\n",
        "#\n",
        "path = FlattenLayer(inputt)\n",
        "\n",
        "for i in range(0,5):\n",
        "  LayerDense1 = keras.layers.Dense(12, activation=None, use_bias=True, kernel_initializer='glorot_uniform')\n",
        "  path = LayerDense1(path)\n",
        "  LayerPReLU1 = keras.layers.PReLU(alpha_initializer='zeros', shared_axes=None)\n",
        "  path = LayerPReLU1(path)\n",
        "#\n",
        "LayerDenseN = keras.layers.Dense(1,activation=None, use_bias=True, kernel_initializer='glorot_uniform')\n",
        "output = LayerDenseN(path)\n",
        "##---------------------------------\n",
        "## Creation of TensorFlow Model\n",
        "##---------------------------------\n",
        "covidModel = keras.Model(inputt, output, name='covidEstimatior')\n",
        "#\n",
        "covidModel.summary() # Display summary\n",
        "#\n",
        "##Włączenia procesu uczenia\n",
        "#\n",
        "rmsOptimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "#\n",
        "covidModel.compile(optimizer=rmsOptimizer,loss=keras.losses.mean_absolute_error)\n",
        "#covidModel.compile(optimizer=rmsOptimizer,loss=keras.losses.binary_crossentropy,metrics=['accuracy'])\n",
        "#\n",
        "\n",
        "covidModel.fit(BazaVec, BazaAns, epochs=150, batch_size=10, shuffle=True)\n",
        "covidModel.save('covid.h5')\n",
        "##Przetestować / użyć sieci\n",
        "\n",
        "BazaVecW = BazaVec[z1-1:z1,:,:]\n",
        "covid = covidModel.predict(BazaVecW)\n",
        "print((covid[0]+1)*max_answer)\n",
        "tekst=open('Przewidywania.txt', 'w')\n",
        "tekst.write(\"Przewidywana liczba przypadków na dzień 11.05.2020 r. w Polsce to\"+ str( (covid[0]+1)*max_answer))\n",
        "tekst.close()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n",
            "/gdrive/My Drive/Colab Notebooks/baza\n",
            "Name: tensorflow\n",
            "Version: 1.14.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: wheel, numpy, protobuf, grpcio, termcolor, google-pasta, wrapt, gast, tensorflow-estimator, six, tensorboard, absl-py, astor, keras-applications, keras-preprocessing\n",
            "Required-by: fancyimpute\n",
            "Name: Keras\n",
            "Version: 2.2.4\n",
            "Summary: Deep Learning for humans\n",
            "Home-page: https://github.com/keras-team/keras\n",
            "Author: Francois Chollet\n",
            "Author-email: francois.chollet@gmail.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: scipy, six, numpy, pyyaml, keras-applications, keras-preprocessing, h5py\n",
            "Required-by: textgenrnn, keras-vis, kapre, fancyimpute\n",
            "Requirement already satisfied: tensorflow==1.14 in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.28.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.18.4)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.34.2)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14) (46.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.18.4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "272.5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "wejscie (InputLayer)         (None, 7, 1)              0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 7)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 12)                96        \n",
            "_________________________________________________________________\n",
            "p_re_lu_1 (PReLU)            (None, 12)                12        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 12)                156       \n",
            "_________________________________________________________________\n",
            "p_re_lu_2 (PReLU)            (None, 12)                12        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 12)                156       \n",
            "_________________________________________________________________\n",
            "p_re_lu_3 (PReLU)            (None, 12)                12        \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 12)                156       \n",
            "_________________________________________________________________\n",
            "p_re_lu_4 (PReLU)            (None, 12)                12        \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 12)                156       \n",
            "_________________________________________________________________\n",
            "p_re_lu_5 (PReLU)            (None, 12)                12        \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 13        \n",
            "=================================================================\n",
            "Total params: 793\n",
            "Trainable params: 793\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/150\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.3595\n",
            "Epoch 2/150\n",
            "63/63 [==============================] - 0s 262us/step - loss: 0.3245\n",
            "Epoch 3/150\n",
            "63/63 [==============================] - 0s 252us/step - loss: 0.2905\n",
            "Epoch 4/150\n",
            "63/63 [==============================] - 0s 244us/step - loss: 0.2507\n",
            "Epoch 5/150\n",
            "63/63 [==============================] - 0s 271us/step - loss: 0.2141\n",
            "Epoch 6/150\n",
            "63/63 [==============================] - 0s 280us/step - loss: 0.1864\n",
            "Epoch 7/150\n",
            "63/63 [==============================] - 0s 285us/step - loss: 0.1737\n",
            "Epoch 8/150\n",
            "63/63 [==============================] - 0s 294us/step - loss: 0.1713\n",
            "Epoch 9/150\n",
            "63/63 [==============================] - 0s 236us/step - loss: 0.1674\n",
            "Epoch 10/150\n",
            "63/63 [==============================] - 0s 250us/step - loss: 0.1629\n",
            "Epoch 11/150\n",
            "63/63 [==============================] - 0s 252us/step - loss: 0.1622\n",
            "Epoch 12/150\n",
            "63/63 [==============================] - 0s 296us/step - loss: 0.1606\n",
            "Epoch 13/150\n",
            "63/63 [==============================] - 0s 490us/step - loss: 0.1568\n",
            "Epoch 14/150\n",
            "63/63 [==============================] - 0s 265us/step - loss: 0.1560\n",
            "Epoch 15/150\n",
            "63/63 [==============================] - 0s 235us/step - loss: 0.1554\n",
            "Epoch 16/150\n",
            "63/63 [==============================] - 0s 260us/step - loss: 0.1538\n",
            "Epoch 17/150\n",
            "63/63 [==============================] - 0s 253us/step - loss: 0.1528\n",
            "Epoch 18/150\n",
            "63/63 [==============================] - 0s 269us/step - loss: 0.1503\n",
            "Epoch 19/150\n",
            "63/63 [==============================] - 0s 214us/step - loss: 0.1512\n",
            "Epoch 20/150\n",
            "63/63 [==============================] - 0s 280us/step - loss: 0.1488\n",
            "Epoch 21/150\n",
            "63/63 [==============================] - 0s 235us/step - loss: 0.1466\n",
            "Epoch 22/150\n",
            "63/63 [==============================] - 0s 238us/step - loss: 0.1477\n",
            "Epoch 23/150\n",
            "63/63 [==============================] - 0s 243us/step - loss: 0.1439\n",
            "Epoch 24/150\n",
            "63/63 [==============================] - 0s 245us/step - loss: 0.1423\n",
            "Epoch 25/150\n",
            "63/63 [==============================] - 0s 256us/step - loss: 0.1413\n",
            "Epoch 26/150\n",
            "63/63 [==============================] - 0s 276us/step - loss: 0.1392\n",
            "Epoch 27/150\n",
            "63/63 [==============================] - 0s 321us/step - loss: 0.1360\n",
            "Epoch 28/150\n",
            "63/63 [==============================] - 0s 303us/step - loss: 0.1357\n",
            "Epoch 29/150\n",
            "63/63 [==============================] - 0s 314us/step - loss: 0.1395\n",
            "Epoch 30/150\n",
            "63/63 [==============================] - 0s 311us/step - loss: 0.1382\n",
            "Epoch 31/150\n",
            "63/63 [==============================] - 0s 259us/step - loss: 0.1391\n",
            "Epoch 32/150\n",
            "63/63 [==============================] - 0s 260us/step - loss: 0.1465\n",
            "Epoch 33/150\n",
            "63/63 [==============================] - 0s 270us/step - loss: 0.1352\n",
            "Epoch 34/150\n",
            "63/63 [==============================] - 0s 283us/step - loss: 0.1387\n",
            "Epoch 35/150\n",
            "63/63 [==============================] - 0s 245us/step - loss: 0.1363\n",
            "Epoch 36/150\n",
            "63/63 [==============================] - 0s 265us/step - loss: 0.1340\n",
            "Epoch 37/150\n",
            "63/63 [==============================] - 0s 225us/step - loss: 0.1335\n",
            "Epoch 38/150\n",
            "63/63 [==============================] - 0s 259us/step - loss: 0.1325\n",
            "Epoch 39/150\n",
            "63/63 [==============================] - 0s 250us/step - loss: 0.1372\n",
            "Epoch 40/150\n",
            "63/63 [==============================] - 0s 234us/step - loss: 0.1338\n",
            "Epoch 41/150\n",
            "63/63 [==============================] - 0s 305us/step - loss: 0.1330\n",
            "Epoch 42/150\n",
            "63/63 [==============================] - 0s 252us/step - loss: 0.1325\n",
            "Epoch 43/150\n",
            "63/63 [==============================] - 0s 224us/step - loss: 0.1326\n",
            "Epoch 44/150\n",
            "63/63 [==============================] - 0s 305us/step - loss: 0.1311\n",
            "Epoch 45/150\n",
            "63/63 [==============================] - 0s 290us/step - loss: 0.1312\n",
            "Epoch 46/150\n",
            "63/63 [==============================] - 0s 251us/step - loss: 0.1297\n",
            "Epoch 47/150\n",
            "63/63 [==============================] - 0s 287us/step - loss: 0.1295\n",
            "Epoch 48/150\n",
            "63/63 [==============================] - 0s 264us/step - loss: 0.1315\n",
            "Epoch 49/150\n",
            "63/63 [==============================] - 0s 372us/step - loss: 0.1325\n",
            "Epoch 50/150\n",
            "63/63 [==============================] - 0s 306us/step - loss: 0.1361\n",
            "Epoch 51/150\n",
            "63/63 [==============================] - 0s 249us/step - loss: 0.1393\n",
            "Epoch 52/150\n",
            "63/63 [==============================] - 0s 246us/step - loss: 0.1371\n",
            "Epoch 53/150\n",
            "63/63 [==============================] - 0s 240us/step - loss: 0.1371\n",
            "Epoch 54/150\n",
            "63/63 [==============================] - 0s 253us/step - loss: 0.1307\n",
            "Epoch 55/150\n",
            "63/63 [==============================] - 0s 246us/step - loss: 0.1312\n",
            "Epoch 56/150\n",
            "63/63 [==============================] - 0s 268us/step - loss: 0.1291\n",
            "Epoch 57/150\n",
            "63/63 [==============================] - 0s 259us/step - loss: 0.1275\n",
            "Epoch 58/150\n",
            "63/63 [==============================] - 0s 251us/step - loss: 0.1293\n",
            "Epoch 59/150\n",
            "63/63 [==============================] - 0s 294us/step - loss: 0.1265\n",
            "Epoch 60/150\n",
            "63/63 [==============================] - 0s 286us/step - loss: 0.1278\n",
            "Epoch 61/150\n",
            "63/63 [==============================] - 0s 317us/step - loss: 0.1282\n",
            "Epoch 62/150\n",
            "63/63 [==============================] - 0s 241us/step - loss: 0.1337\n",
            "Epoch 63/150\n",
            "63/63 [==============================] - 0s 223us/step - loss: 0.1328\n",
            "Epoch 64/150\n",
            "63/63 [==============================] - 0s 244us/step - loss: 0.1394\n",
            "Epoch 65/150\n",
            "63/63 [==============================] - 0s 213us/step - loss: 0.1293\n",
            "Epoch 66/150\n",
            "63/63 [==============================] - 0s 302us/step - loss: 0.1324\n",
            "Epoch 67/150\n",
            "63/63 [==============================] - 0s 250us/step - loss: 0.1261\n",
            "Epoch 68/150\n",
            "63/63 [==============================] - 0s 241us/step - loss: 0.1299\n",
            "Epoch 69/150\n",
            "63/63 [==============================] - 0s 324us/step - loss: 0.1261\n",
            "Epoch 70/150\n",
            "63/63 [==============================] - 0s 259us/step - loss: 0.1251\n",
            "Epoch 71/150\n",
            "63/63 [==============================] - 0s 236us/step - loss: 0.1255\n",
            "Epoch 72/150\n",
            "63/63 [==============================] - 0s 250us/step - loss: 0.1329\n",
            "Epoch 73/150\n",
            "63/63 [==============================] - 0s 218us/step - loss: 0.1227\n",
            "Epoch 74/150\n",
            "63/63 [==============================] - 0s 231us/step - loss: 0.1227\n",
            "Epoch 75/150\n",
            "63/63 [==============================] - 0s 224us/step - loss: 0.1231\n",
            "Epoch 76/150\n",
            "63/63 [==============================] - 0s 247us/step - loss: 0.1206\n",
            "Epoch 77/150\n",
            "63/63 [==============================] - 0s 208us/step - loss: 0.1226\n",
            "Epoch 78/150\n",
            "63/63 [==============================] - 0s 202us/step - loss: 0.1228\n",
            "Epoch 79/150\n",
            "63/63 [==============================] - 0s 366us/step - loss: 0.1220\n",
            "Epoch 80/150\n",
            "63/63 [==============================] - 0s 249us/step - loss: 0.1230\n",
            "Epoch 81/150\n",
            "63/63 [==============================] - 0s 204us/step - loss: 0.1204\n",
            "Epoch 82/150\n",
            "63/63 [==============================] - 0s 261us/step - loss: 0.1184\n",
            "Epoch 83/150\n",
            "63/63 [==============================] - 0s 254us/step - loss: 0.1183\n",
            "Epoch 84/150\n",
            "63/63 [==============================] - 0s 201us/step - loss: 0.1173\n",
            "Epoch 85/150\n",
            "63/63 [==============================] - 0s 213us/step - loss: 0.1173\n",
            "Epoch 86/150\n",
            "63/63 [==============================] - 0s 207us/step - loss: 0.1173\n",
            "Epoch 87/150\n",
            "63/63 [==============================] - 0s 246us/step - loss: 0.1214\n",
            "Epoch 88/150\n",
            "63/63 [==============================] - 0s 271us/step - loss: 0.1275\n",
            "Epoch 89/150\n",
            "63/63 [==============================] - 0s 297us/step - loss: 0.1169\n",
            "Epoch 90/150\n",
            "63/63 [==============================] - 0s 255us/step - loss: 0.1158\n",
            "Epoch 91/150\n",
            "63/63 [==============================] - 0s 256us/step - loss: 0.1188\n",
            "Epoch 92/150\n",
            "63/63 [==============================] - 0s 271us/step - loss: 0.1191\n",
            "Epoch 93/150\n",
            "63/63 [==============================] - 0s 313us/step - loss: 0.1190\n",
            "Epoch 94/150\n",
            "63/63 [==============================] - 0s 298us/step - loss: 0.1242\n",
            "Epoch 95/150\n",
            "63/63 [==============================] - 0s 273us/step - loss: 0.1136\n",
            "Epoch 96/150\n",
            "63/63 [==============================] - 0s 253us/step - loss: 0.1206\n",
            "Epoch 97/150\n",
            "63/63 [==============================] - 0s 273us/step - loss: 0.1197\n",
            "Epoch 98/150\n",
            "63/63 [==============================] - 0s 305us/step - loss: 0.1135\n",
            "Epoch 99/150\n",
            "63/63 [==============================] - 0s 287us/step - loss: 0.1144\n",
            "Epoch 100/150\n",
            "63/63 [==============================] - 0s 266us/step - loss: 0.1113\n",
            "Epoch 101/150\n",
            "63/63 [==============================] - 0s 243us/step - loss: 0.1112\n",
            "Epoch 102/150\n",
            "63/63 [==============================] - 0s 264us/step - loss: 0.1138\n",
            "Epoch 103/150\n",
            "63/63 [==============================] - 0s 263us/step - loss: 0.1080\n",
            "Epoch 104/150\n",
            "63/63 [==============================] - 0s 241us/step - loss: 0.1099\n",
            "Epoch 105/150\n",
            "63/63 [==============================] - 0s 328us/step - loss: 0.1067\n",
            "Epoch 106/150\n",
            "63/63 [==============================] - 0s 311us/step - loss: 0.1076\n",
            "Epoch 107/150\n",
            "63/63 [==============================] - 0s 340us/step - loss: 0.1055\n",
            "Epoch 108/150\n",
            "63/63 [==============================] - 0s 260us/step - loss: 0.1069\n",
            "Epoch 109/150\n",
            "63/63 [==============================] - 0s 285us/step - loss: 0.1061\n",
            "Epoch 110/150\n",
            "63/63 [==============================] - 0s 254us/step - loss: 0.1023\n",
            "Epoch 111/150\n",
            "63/63 [==============================] - 0s 248us/step - loss: 0.1026\n",
            "Epoch 112/150\n",
            "63/63 [==============================] - 0s 253us/step - loss: 0.1036\n",
            "Epoch 113/150\n",
            "63/63 [==============================] - 0s 250us/step - loss: 0.1073\n",
            "Epoch 114/150\n",
            "63/63 [==============================] - 0s 259us/step - loss: 0.1047\n",
            "Epoch 115/150\n",
            "63/63 [==============================] - 0s 256us/step - loss: 0.1033\n",
            "Epoch 116/150\n",
            "63/63 [==============================] - 0s 257us/step - loss: 0.1052\n",
            "Epoch 117/150\n",
            "63/63 [==============================] - 0s 249us/step - loss: 0.1058\n",
            "Epoch 118/150\n",
            "63/63 [==============================] - 0s 247us/step - loss: 0.1011\n",
            "Epoch 119/150\n",
            "63/63 [==============================] - 0s 249us/step - loss: 0.0973\n",
            "Epoch 120/150\n",
            "63/63 [==============================] - 0s 247us/step - loss: 0.0982\n",
            "Epoch 121/150\n",
            "63/63 [==============================] - 0s 250us/step - loss: 0.0970\n",
            "Epoch 122/150\n",
            "63/63 [==============================] - 0s 249us/step - loss: 0.1063\n",
            "Epoch 123/150\n",
            "63/63 [==============================] - 0s 242us/step - loss: 0.0978\n",
            "Epoch 124/150\n",
            "63/63 [==============================] - 0s 234us/step - loss: 0.0988\n",
            "Epoch 125/150\n",
            "63/63 [==============================] - 0s 271us/step - loss: 0.0988\n",
            "Epoch 126/150\n",
            "63/63 [==============================] - 0s 314us/step - loss: 0.0971\n",
            "Epoch 127/150\n",
            "63/63 [==============================] - 0s 267us/step - loss: 0.0916\n",
            "Epoch 128/150\n",
            "63/63 [==============================] - 0s 219us/step - loss: 0.0935\n",
            "Epoch 129/150\n",
            "63/63 [==============================] - 0s 280us/step - loss: 0.0944\n",
            "Epoch 130/150\n",
            "63/63 [==============================] - 0s 220us/step - loss: 0.0965\n",
            "Epoch 131/150\n",
            "63/63 [==============================] - 0s 237us/step - loss: 0.0977\n",
            "Epoch 132/150\n",
            "63/63 [==============================] - 0s 228us/step - loss: 0.0888\n",
            "Epoch 133/150\n",
            "63/63 [==============================] - 0s 231us/step - loss: 0.0873\n",
            "Epoch 134/150\n",
            "63/63 [==============================] - 0s 256us/step - loss: 0.0902\n",
            "Epoch 135/150\n",
            "63/63 [==============================] - 0s 256us/step - loss: 0.0894\n",
            "Epoch 136/150\n",
            "63/63 [==============================] - 0s 250us/step - loss: 0.0865\n",
            "Epoch 137/150\n",
            "63/63 [==============================] - 0s 245us/step - loss: 0.0842\n",
            "Epoch 138/150\n",
            "63/63 [==============================] - 0s 270us/step - loss: 0.0821\n",
            "Epoch 139/150\n",
            "63/63 [==============================] - 0s 276us/step - loss: 0.0835\n",
            "Epoch 140/150\n",
            "63/63 [==============================] - 0s 252us/step - loss: 0.0865\n",
            "Epoch 141/150\n",
            "63/63 [==============================] - 0s 266us/step - loss: 0.0812\n",
            "Epoch 142/150\n",
            "63/63 [==============================] - 0s 265us/step - loss: 0.0808\n",
            "Epoch 143/150\n",
            "63/63 [==============================] - 0s 274us/step - loss: 0.0816\n",
            "Epoch 144/150\n",
            "63/63 [==============================] - 0s 260us/step - loss: 0.0805\n",
            "Epoch 145/150\n",
            "63/63 [==============================] - 0s 258us/step - loss: 0.0814\n",
            "Epoch 146/150\n",
            "63/63 [==============================] - 0s 247us/step - loss: 0.0829\n",
            "Epoch 147/150\n",
            "63/63 [==============================] - 0s 255us/step - loss: 0.0980\n",
            "Epoch 148/150\n",
            "63/63 [==============================] - 0s 267us/step - loss: 0.1031\n",
            "Epoch 149/150\n",
            "63/63 [==============================] - 0s 262us/step - loss: 0.0822\n",
            "Epoch 150/150\n",
            "63/63 [==============================] - 0s 264us/step - loss: 0.0908\n",
            "[357.877]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9ylWTaViPG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}